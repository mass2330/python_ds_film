{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, il s'agira de faire la partie modélisation du projet. Après un nettoyage de la base des critiques en utilisant des techniques de NLP, nous allons réliaser des modèles permettant de prédire la positivité ou négativité de la critique. Ensuite, nous comparerons les résultats obtenus à un modèle pré entrainé sur des textes en français, issue de BERT, qu'on aura au préalable fit sur nos données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classique python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing, naive_bayes, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# CamemBERT\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage + NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du df enregistré après le scraping des critiques sur Allociné\n",
    "df_critiques = pd.read_csv(\"df_critiques.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques = df_critiques.dropna(subset=['Critique'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va commencer par convertir les notes en float et extraire la date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les notes en float\n",
    "df_critiques['Note_critique_float'] = df_critiques['Note de la critique'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Extraire la date\n",
    "df_critiques['new_date1'] = df_critiques['Date de publication'].str.replace('Publiée le', '').apply(lambda x: ' '.join([french_to_english_month(word) for word in str(x).split()]) if pd.notna(x) else np.nan)\n",
    "\n",
    "# Utiliser dateutil.parser.parse pour convertir les dates en objets datetime\n",
    "df_critiques['new_date'] = df_critiques['new_date1'].apply(lambda x: parser.parse(x, dayfirst=True) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "colonnes_a_supprimer = [\"new_date1\", \"Date de publication\",'Note de la critique']\n",
    "df_critiques = df_critiques.drop(columns=colonnes_a_supprimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rôle de la fonction : Définir une polarité à partir des notes des commentaires\n",
    "# Entrée : note d'un commentaire\n",
    "# Sortie : Retourne une polarité suivant la note\n",
    "\n",
    "def find_polarity(note_commentaire):\n",
    "    if float(note_commentaire) <= 3:\n",
    "        return 0 # Négatif\n",
    "    else:\n",
    "        return 1 # Positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques['Polarité_réelle'] = df_critiques['Note_critique_float'].apply(find_polarity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rôle de la fonction : Nettoyer les critiques dans le but de faire du NLP\n",
    "# Entrée : Critique du df\n",
    "# Sortie : Renvoie la critique nettoyé\n",
    "\n",
    "\n",
    "def clean_text_french(critique):\n",
    "    # Assurez-vous que la colonne de commentaire est de type chaîne de caractères\n",
    "    critique = critique.astype(str)\n",
    "\n",
    "    # Convertir le texte en minuscules\n",
    "    critique = critique.str.lower()\n",
    "\n",
    "    # Tokenisation des mots\n",
    "    critique = critique.apply(word_tokenize, language='french')\n",
    "\n",
    "    # Supprimer la ponctuation et les caractères spéciaux\n",
    "    critique = critique.apply(lambda tokens: [word for word in tokens if word.isalnum()])\n",
    "\n",
    "    # Supprimer les mots vides\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    critique = critique.apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "    # Racinisation (stemming)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    critique = critique.apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "    # Rejoindre les tokens en une seule chaîne de texte\n",
    "    critique = critique.apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "\n",
    "    return critique\n",
    "\n",
    "# python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques = df_critiques[df_critiques['Critique'].astype(str).apply(lambda x: isinstance(x, str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_critiques['Critique_nettoye'] = pd.DataFrame(clean_text_french(df_critiques['Critique']))\n",
    "\n",
    "# CPU times: user 20min, sys: 16.5 s, total: 20min 17s\n",
    "# Wall time: 20min 17s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait attention aux valeurs nulles\n",
    "df_critiques = df_critiques.dropna(subset=['Critique_nettoye'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rôle : \n",
    "# Entrée : \n",
    "# Sortie : \n",
    "\n",
    "\n",
    "def compter_mots(critique):\n",
    "    if isinstance(critique, str):\n",
    "        mots = critique.split()\n",
    "        return len(mots)\n",
    "    else:\n",
    "        # Si la valeur n'est pas une chaîne de caractères, vous pouvez choisir de traiter cela d'une manière particulière.\n",
    "        # Par exemple, vous pourriez retourner 0 ou une autre valeur appropriée.\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction pour compter les mots à la colonne 'Critique' et créer une nouvelle colonne 'nb_mots'\n",
    "df_critiques['nb_mots'] = df_critiques['Critique'].apply(compter_mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "\n",
    "# Rôle de la fonction : Utilisation de text blob pour atrrtibuer une polarité (postitive ou négative) à un commentaire\n",
    "# Entrée : Commentaire\n",
    "# Sortie : Retourne une polarité du commentaire\n",
    "\n",
    "def attribuer_note(commentaire):\n",
    "    #\n",
    "    vs = tb(commentaire).sentiment[0]\n",
    "    if vs > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Appliquer la fonction pour attribuer une note à chaque commentaire\n",
    "df_critiques['Text_blob_critique_nettoyé'] = df_critiques['Critique_nettoye'].apply(attribuer_note)\n",
    "\n",
    "# CPU times: user 3min 46s, sys: 188 ms, total: 3min 46s\n",
    "# Wall time: 3min 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques.to_csv(\"df_critiques_modif.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques = pd.read_csv(\"df_critiques_modif.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques = df_critiques.dropna(subset=['Critique_nettoy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques = df_critiques[df_critiques['nb_mots'] <= 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_allocine</th>\n",
       "      <th>Critique</th>\n",
       "      <th>Note_critique_float</th>\n",
       "      <th>new_date</th>\n",
       "      <th>Polarité_réelle</th>\n",
       "      <th>Critique_nettoy</th>\n",
       "      <th>nb_mots</th>\n",
       "      <th>Text_blob_critique_nettoyé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239331</td>\n",
       "      <td>\"10.000 days\" est à la base une série créée pa...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2015-10-05</td>\n",
       "      <td>0</td>\n",
       "      <td>day base série créée eric small réalise film o...</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>239331</td>\n",
       "      <td>Les effets spéciaux bas de gamme mettent les a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>effets spéciaux ba gamme mettent acteurs situa...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239331</td>\n",
       "      <td>10,000 somnifères. A peine ça commence qu'un g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>0</td>\n",
       "      <td>somnifères a peine ça commence gar raconte lif...</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>239331</td>\n",
       "      <td>\"10 000 days\" est d'une nullité abyssale. Tout...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>0</td>\n",
       "      <td>10 000 day nullité abyssale tout concourt fair...</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>239331</td>\n",
       "      <td>Beaucoup de parlotes, pas d'action ni d'effets...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>beaucoup parlotes ni spéciaux histoire molle t...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801583</th>\n",
       "      <td>146631</td>\n",
       "      <td>J'avais beaucoup aimé le premier Zoolander, fo...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-05-21</td>\n",
       "      <td>0</td>\n",
       "      <td>beaucoup aimé premier zoolander fort quantité ...</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801585</th>\n",
       "      <td>146631</td>\n",
       "      <td>La sauce ne prend malheureusement pas. \"Zoolan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-05-17</td>\n",
       "      <td>0</td>\n",
       "      <td>sauce prend malheureusement zoolander devenu c...</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801586</th>\n",
       "      <td>146631</td>\n",
       "      <td>la demi étoiles et pour le casting avec des ac...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>demi étoiles casting acteurs fou film a humour...</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801587</th>\n",
       "      <td>146631</td>\n",
       "      <td>Pas aussi mauvais...Pas très Drôle mais amusan...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>0</td>\n",
       "      <td>aussi mauvais très drôle amusant bien filmé te...</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801588</th>\n",
       "      <td>146631</td>\n",
       "      <td>ma fait halluciner , j'ai trouvé ça trop énorm...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-05-13</td>\n",
       "      <td>1</td>\n",
       "      <td>fait halluciner trouvé ça trop énorme look mod...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750789 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_allocine                                           Critique  \\\n",
       "0            239331  \"10.000 days\" est à la base une série créée pa...   \n",
       "1            239331  Les effets spéciaux bas de gamme mettent les a...   \n",
       "2            239331  10,000 somnifères. A peine ça commence qu'un g...   \n",
       "3            239331  \"10 000 days\" est d'une nullité abyssale. Tout...   \n",
       "4            239331  Beaucoup de parlotes, pas d'action ni d'effets...   \n",
       "...             ...                                                ...   \n",
       "801583       146631  J'avais beaucoup aimé le premier Zoolander, fo...   \n",
       "801585       146631  La sauce ne prend malheureusement pas. \"Zoolan...   \n",
       "801586       146631  la demi étoiles et pour le casting avec des ac...   \n",
       "801587       146631  Pas aussi mauvais...Pas très Drôle mais amusan...   \n",
       "801588       146631  ma fait halluciner , j'ai trouvé ça trop énorm...   \n",
       "\n",
       "        Note_critique_float    new_date  Polarité_réelle  \\\n",
       "0                       0.5  2015-10-05                0   \n",
       "1                       1.0  2015-12-12                0   \n",
       "2                       1.0  2017-01-10                0   \n",
       "3                       0.5  2016-05-24                0   \n",
       "4                       1.5  2017-01-01                0   \n",
       "...                     ...         ...              ...   \n",
       "801583                  2.0  2016-05-21                0   \n",
       "801585                  2.0  2016-05-17                0   \n",
       "801586                  0.5  2020-07-15                0   \n",
       "801587                  2.5  2016-03-21                0   \n",
       "801588                  5.0  2016-05-13                1   \n",
       "\n",
       "                                          Critique_nettoy  nb_mots  \\\n",
       "0       day base série créée eric small réalise film o...      129   \n",
       "1       effets spéciaux ba gamme mettent acteurs situa...       18   \n",
       "2       somnifères a peine ça commence gar raconte lif...       45   \n",
       "3       10 000 day nullité abyssale tout concourt fair...       84   \n",
       "4       beaucoup parlotes ni spéciaux histoire molle t...       22   \n",
       "...                                                   ...      ...   \n",
       "801583  beaucoup aimé premier zoolander fort quantité ...      115   \n",
       "801585  sauce prend malheureusement zoolander devenu c...       72   \n",
       "801586  demi étoiles casting acteurs fou film a humour...       43   \n",
       "801587  aussi mauvais très drôle amusant bien filmé te...       46   \n",
       "801588  fait halluciner trouvé ça trop énorme look mod...       65   \n",
       "\n",
       "        Text_blob_critique_nettoyé  \n",
       "0                                0  \n",
       "1                                0  \n",
       "2                                0  \n",
       "3                                0  \n",
       "4                                0  \n",
       "...                            ...  \n",
       "801583                           1  \n",
       "801585                           0  \n",
       "801586                           0  \n",
       "801587                           1  \n",
       "801588                           1  \n",
       "\n",
       "[750789 rows x 8 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_critiques_mod = df_critiques.sample(n=100000, random_state=2)  # Utilisez un random_state pour la reproductibilité\n",
    "\n",
    "# 40 echantillons à 100 000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Polarité_réelle\n",
       "0    45740\n",
       "1    54260\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_critiques_mod.groupby('Polarité_réelle').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_allocine</th>\n",
       "      <th>Critique</th>\n",
       "      <th>Note_critique_float</th>\n",
       "      <th>new_date</th>\n",
       "      <th>Polarité_réelle</th>\n",
       "      <th>Critique_nettoy</th>\n",
       "      <th>nb_mots</th>\n",
       "      <th>Text_blob_critique_nettoyé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50107</th>\n",
       "      <td>237763</td>\n",
       "      <td>Film monotone et sans surprise qui se contente...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>film monotone sans surprise contente faire pub...</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501267</th>\n",
       "      <td>229359</td>\n",
       "      <td>À l'image de son personnage : sans envergure, ...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2016-11-13</td>\n",
       "      <td>0</td>\n",
       "      <td>personnage sans envergure naïf film rapidement...</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262765</th>\n",
       "      <td>114782</td>\n",
       "      <td>un vrai bonheur visuelun vrai drame, humain, p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-05-29</td>\n",
       "      <td>1</td>\n",
       "      <td>vrai bonheur visuelun vrai drame humain person...</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380156</th>\n",
       "      <td>239713</td>\n",
       "      <td>J'y suis allée un peu au hasard d'un horaire d...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>allée peu hasard horaire ciné excellente surpr...</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329420</th>\n",
       "      <td>229070</td>\n",
       "      <td>Avec ses 6 nominations aux Oscars et sa promot...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>1</td>\n",
       "      <td>6 nomination oscar promotion basée histoire vr...</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679484</th>\n",
       "      <td>144195</td>\n",
       "      <td>Very Bad Cops : Je ne comprends pas, a mon pre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2012-05-08</td>\n",
       "      <td>1</td>\n",
       "      <td>very bad cop comprends a premier visionnage fi...</td>\n",
       "      <td>276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657630</th>\n",
       "      <td>179426</td>\n",
       "      <td>Salut ,Amateur de films épouvante/horreur , j'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-09-18</td>\n",
       "      <td>0</td>\n",
       "      <td>salut amateur film très déçu celui ci première...</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775080</th>\n",
       "      <td>182075</td>\n",
       "      <td>J'étais impatient de voir le film au vu de son...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>0</td>\n",
       "      <td>impatient voir film vu déception film constamm...</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459589</th>\n",
       "      <td>265585</td>\n",
       "      <td>On a bien la patte \"Zemeckis\" avec les effets ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>0</td>\n",
       "      <td>a bien patte zemeckis effets réalisation musiq...</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688890</th>\n",
       "      <td>170893</td>\n",
       "      <td>Un scénario bien facile et un jeu d'acteur un ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>1</td>\n",
       "      <td>scénario bien facile jeu brin psychotique fero...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_allocine                                           Critique  \\\n",
       "50107        237763  Film monotone et sans surprise qui se contente...   \n",
       "501267       229359  À l'image de son personnage : sans envergure, ...   \n",
       "262765       114782  un vrai bonheur visuelun vrai drame, humain, p...   \n",
       "380156       239713  J'y suis allée un peu au hasard d'un horaire d...   \n",
       "329420       229070  Avec ses 6 nominations aux Oscars et sa promot...   \n",
       "...             ...                                                ...   \n",
       "679484       144195  Very Bad Cops : Je ne comprends pas, a mon pre...   \n",
       "657630       179426  Salut ,Amateur de films épouvante/horreur , j'...   \n",
       "775080       182075  J'étais impatient de voir le film au vu de son...   \n",
       "459589       265585  On a bien la patte \"Zemeckis\" avec les effets ...   \n",
       "688890       170893  Un scénario bien facile et un jeu d'acteur un ...   \n",
       "\n",
       "        Note_critique_float    new_date  Polarité_réelle  \\\n",
       "50107                   0.5  2018-03-17                0   \n",
       "501267                  1.5  2016-11-13                0   \n",
       "262765                  5.0  2018-05-29                1   \n",
       "380156                  4.0  2017-01-23                1   \n",
       "329420                  4.0  2018-02-05                1   \n",
       "...                     ...         ...              ...   \n",
       "679484                  5.0  2012-05-08                1   \n",
       "657630                  1.0  2010-09-18                0   \n",
       "775080                  1.5  2012-04-16                0   \n",
       "459589                  2.0  2020-12-29                0   \n",
       "688890                  3.5  2012-10-19                1   \n",
       "\n",
       "                                          Critique_nettoy  nb_mots  \\\n",
       "50107   film monotone sans surprise contente faire pub...       89   \n",
       "501267  personnage sans envergure naïf film rapidement...       93   \n",
       "262765  vrai bonheur visuelun vrai drame humain person...       54   \n",
       "380156  allée peu hasard horaire ciné excellente surpr...       55   \n",
       "329420  6 nomination oscar promotion basée histoire vr...      170   \n",
       "...                                                   ...      ...   \n",
       "679484  very bad cop comprends a premier visionnage fi...      276   \n",
       "657630  salut amateur film très déçu celui ci première...       76   \n",
       "775080  impatient voir film vu déception film constamm...      105   \n",
       "459589  a bien patte zemeckis effets réalisation musiq...       43   \n",
       "688890  scénario bien facile jeu brin psychotique fero...       50   \n",
       "\n",
       "        Text_blob_critique_nettoyé  \n",
       "50107                            0  \n",
       "501267                           1  \n",
       "262765                           1  \n",
       "380156                           1  \n",
       "329420                           1  \n",
       "...                            ...  \n",
       "679484                           1  \n",
       "657630                           0  \n",
       "775080                           0  \n",
       "459589                           1  \n",
       "688890                           1  \n",
       "\n",
       "[100000 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_critiques_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prépare nos  jeux de test et d'entrainement pour le countvectorizer et pour le ML\n",
    "\n",
    "critique_train, critique_test, polarite_train, polarite_test = train_test_split(df_critiques_mod['Critique_nettoy'], df_critiques_mod['Polarité_réelle'], test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer \n",
    "# Check limit colonne\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv_train_features = cv.fit_transform(critique_train)\n",
    "cv_test_features = cv.transform(critique_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Rôle de la fonction : Fit + résultat du modèle\n",
    "# Entrée : Modèle considéré, les 4 sets de test et d'entrainement\n",
    "# Sortie : les résultats du modèles\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "\n",
    "def fitting_predictions(model, train_features, train_labels, test_features):\n",
    "    model.fit(train_features, train_labels)\n",
    "    predictions = model.predict(test_features)\n",
    "    return predictions\n",
    "\n",
    "def print_metrics(test_labels, predictions):\n",
    "    # On stock nos données dans un dictionnaire\n",
    "    results = {\n",
    "        'Accuracy': metrics.accuracy_score(test_labels, predictions),\n",
    "        'Precision': metrics.precision_score(test_labels, predictions),\n",
    "        'Recall': metrics.recall_score(test_labels, predictions),\n",
    "        'F1 Score': metrics.f1_score(test_labels, predictions),\n",
    "        'Classification Report': metrics.classification_report(test_labels, predictions),\n",
    "        'Confusion Matrix': metrics.confusion_matrix(test_labels, predictions)\n",
    "    }\n",
    "    \n",
    "    # Print metriques\n",
    "    print(\"\\nAccuracy:\", results['Accuracy'])\n",
    "    print(\"Precision:\", results['Precision'])\n",
    "    print(\"Recall:\", results['Recall'])\n",
    "    print(\"F1 Score:\", results['F1 Score'])\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['Classification Report'])\n",
    "\n",
    "    # Display confusion matrix as a heatmap using seaborn\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model_final(model, train_features, train_labels, test_features, test_labels):\n",
    "    \n",
    "    predictions = fitting_predictions(model, train_features, train_labels, test_features)\n",
    "\n",
    "    # Cross-validation \n",
    "    cv_scores = cross_val_score(model, train_features, train_labels, cv=StratifiedKFold(n_splits=5), scoring='accuracy')\n",
    "    print(\"Cross-validation Scores:\", cv_scores)\n",
    "    print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "    print_metrics(test_labels, predictions)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultats Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.6695942535119721\n",
      "Precision: 0.6363074169415172\n",
      "Recall: 0.924393337731995\n",
      "F1 Score: 0.7537618409023455\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.36      0.50    340058\n",
      "           1       0.64      0.92      0.75    410731\n",
      "\n",
      "    accuracy                           0.67    750789\n",
      "   macro avg       0.72      0.64      0.63    750789\n",
      "weighted avg       0.71      0.67      0.64    750789\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/python_ds_film/modelisation.ipynb Cell 37\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test_labels\u001b[39m=\u001b[39m df_critiques[\u001b[39m'\u001b[39m\u001b[39mPolarité_réelle\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m predictions \u001b[39m=\u001b[39m df_critiques[\u001b[39m'\u001b[39m\u001b[39mText_blob_critique_nettoyé\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m print_metrics(test_labels, predictions)\n",
      "\u001b[1;32m/home/onyxia/work/python_ds_film/modelisation.ipynb Cell 37\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(results[\u001b[39m'\u001b[39m\u001b[39mClassification Report\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Display confusion matrix as a heatmap using seaborn\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(test_labels, predictions)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m sns\u001b[39m.\u001b[39mheatmap(cm, annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fmt\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBlues\u001b[39m\u001b[39m'\u001b[39m, xticklabels\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m], yticklabels\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "test_labels= df_critiques['Polarité_réelle']\n",
    "predictions = df_critiques['Text_blob_critique_nettoyé']\n",
    "\n",
    "print_metrics(test_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèles ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_lr = LogisticRegression(C=0.5, max_iter=100000)\n",
    "logistic_regression_results = evaluate_model_final(model_lr, cv_train_features, polarite_train, cv_test_features, polarite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Example with Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_results = evaluate_model_final(random_forest, cv_train_features, polarite_train, cv_test_features, polarite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Example with Support Vector Machine\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm_results = evaluate_model_final(svm, cv_train_features, polarite_train, cv_test_features, polarite_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naive Bayes on Count Vectors\n",
    "\n",
    "model_MultinomialNB= naive_bayes.MultinomialNB()\n",
    "MultinomialNB_results = evaluate_model_final(model_MultinomialNB, cv_train_features, polarite_train, cv_test_features, polarite_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_allocine</th>\n",
       "      <th>Critique</th>\n",
       "      <th>Note_critique_float</th>\n",
       "      <th>new_date</th>\n",
       "      <th>Polarité_réelle</th>\n",
       "      <th>Critique_nettoy</th>\n",
       "      <th>nb_mots</th>\n",
       "      <th>Text_blob_critique_nettoyé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50107</th>\n",
       "      <td>237763</td>\n",
       "      <td>Film monotone et sans surprise qui se contente...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>film monotone sans surprise contente faire pub...</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501267</th>\n",
       "      <td>229359</td>\n",
       "      <td>À l'image de son personnage : sans envergure, ...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2016-11-13</td>\n",
       "      <td>0</td>\n",
       "      <td>personnage sans envergure naïf film rapidement...</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262765</th>\n",
       "      <td>114782</td>\n",
       "      <td>un vrai bonheur visuelun vrai drame, humain, p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-05-29</td>\n",
       "      <td>1</td>\n",
       "      <td>vrai bonheur visuelun vrai drame humain person...</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380156</th>\n",
       "      <td>239713</td>\n",
       "      <td>J'y suis allée un peu au hasard d'un horaire d...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>1</td>\n",
       "      <td>allée peu hasard horaire ciné excellente surpr...</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329420</th>\n",
       "      <td>229070</td>\n",
       "      <td>Avec ses 6 nominations aux Oscars et sa promot...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>1</td>\n",
       "      <td>6 nomination oscar promotion basée histoire vr...</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679484</th>\n",
       "      <td>144195</td>\n",
       "      <td>Very Bad Cops : Je ne comprends pas, a mon pre...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2012-05-08</td>\n",
       "      <td>1</td>\n",
       "      <td>very bad cop comprends a premier visionnage fi...</td>\n",
       "      <td>276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657630</th>\n",
       "      <td>179426</td>\n",
       "      <td>Salut ,Amateur de films épouvante/horreur , j'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-09-18</td>\n",
       "      <td>0</td>\n",
       "      <td>salut amateur film très déçu celui ci première...</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775080</th>\n",
       "      <td>182075</td>\n",
       "      <td>J'étais impatient de voir le film au vu de son...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>0</td>\n",
       "      <td>impatient voir film vu déception film constamm...</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459589</th>\n",
       "      <td>265585</td>\n",
       "      <td>On a bien la patte \"Zemeckis\" avec les effets ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>0</td>\n",
       "      <td>a bien patte zemeckis effets réalisation musiq...</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688890</th>\n",
       "      <td>170893</td>\n",
       "      <td>Un scénario bien facile et un jeu d'acteur un ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>1</td>\n",
       "      <td>scénario bien facile jeu brin psychotique fero...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_allocine                                           Critique  \\\n",
       "50107        237763  Film monotone et sans surprise qui se contente...   \n",
       "501267       229359  À l'image de son personnage : sans envergure, ...   \n",
       "262765       114782  un vrai bonheur visuelun vrai drame, humain, p...   \n",
       "380156       239713  J'y suis allée un peu au hasard d'un horaire d...   \n",
       "329420       229070  Avec ses 6 nominations aux Oscars et sa promot...   \n",
       "...             ...                                                ...   \n",
       "679484       144195  Very Bad Cops : Je ne comprends pas, a mon pre...   \n",
       "657630       179426  Salut ,Amateur de films épouvante/horreur , j'...   \n",
       "775080       182075  J'étais impatient de voir le film au vu de son...   \n",
       "459589       265585  On a bien la patte \"Zemeckis\" avec les effets ...   \n",
       "688890       170893  Un scénario bien facile et un jeu d'acteur un ...   \n",
       "\n",
       "        Note_critique_float    new_date  Polarité_réelle  \\\n",
       "50107                   0.5  2018-03-17                0   \n",
       "501267                  1.5  2016-11-13                0   \n",
       "262765                  5.0  2018-05-29                1   \n",
       "380156                  4.0  2017-01-23                1   \n",
       "329420                  4.0  2018-02-05                1   \n",
       "...                     ...         ...              ...   \n",
       "679484                  5.0  2012-05-08                1   \n",
       "657630                  1.0  2010-09-18                0   \n",
       "775080                  1.5  2012-04-16                0   \n",
       "459589                  2.0  2020-12-29                0   \n",
       "688890                  3.5  2012-10-19                1   \n",
       "\n",
       "                                          Critique_nettoy  nb_mots  \\\n",
       "50107   film monotone sans surprise contente faire pub...       89   \n",
       "501267  personnage sans envergure naïf film rapidement...       93   \n",
       "262765  vrai bonheur visuelun vrai drame humain person...       54   \n",
       "380156  allée peu hasard horaire ciné excellente surpr...       55   \n",
       "329420  6 nomination oscar promotion basée histoire vr...      170   \n",
       "...                                                   ...      ...   \n",
       "679484  very bad cop comprends a premier visionnage fi...      276   \n",
       "657630  salut amateur film très déçu celui ci première...       76   \n",
       "775080  impatient voir film vu déception film constamm...      105   \n",
       "459589  a bien patte zemeckis effets réalisation musiq...       43   \n",
       "688890  scénario bien facile jeu brin psychotique fero...       50   \n",
       "\n",
       "        Text_blob_critique_nettoyé  \n",
       "50107                            0  \n",
       "501267                           1  \n",
       "262765                           1  \n",
       "380156                           1  \n",
       "329420                           1  \n",
       "...                            ...  \n",
       "679484                           1  \n",
       "657630                           0  \n",
       "775080                           0  \n",
       "459589                           1  \n",
       "688890                           1  \n",
       "\n",
       "[100000 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_critiques_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "critique = df_critiques_mod['Critique'].values.tolist()\n",
    "polarite = df_critiques_mod['Polarité_réelle'].values.tolist()\n",
    "\n",
    "print(len(critique))\n",
    "print(len(polarite))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "80000\n",
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "split_border = int(len(polarite)*0.8)\n",
    "critique_train, critique_test = critique[:split_border], critique[split_border:]\n",
    "polarite_train, polarite_test = polarite[:split_border], polarite[split_border:]\n",
    "\n",
    "# Charger le modèle Camembert pré-entraîné et le tokenizer\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "model = CamembertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(len(critique_train))\n",
    "print(len(polarite_train))\n",
    "\n",
    "print(len(critique_test))\n",
    "print(len(polarite_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraiter les données\n",
    "def preprocess_data(critique, polarite, tokenizer):\n",
    "    encoded = tokenizer.batch_encode_plus(\n",
    "        critique,\n",
    "        add_special_tokens=False,\n",
    "        padding=True,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    polarite_ = torch.tensor(polarite)\n",
    "\n",
    "    return encoded['input_ids'], encoded['attention_mask'], polarite_\n",
    "\n",
    "    #torch.tensor(polarite_onehot, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 512])\n",
      "torch.Size([80000, 512])\n",
      "torch.Size([80000])\n",
      "torch.Size([20000, 497])\n",
      "torch.Size([20000, 497])\n",
      "torch.Size([20000])\n"
     ]
    }
   ],
   "source": [
    "input_ids_train, attention_mask_train, polarite_train = preprocess_data(critique_train, polarite_train, tokenizer)\n",
    "print(input_ids_train.shape)\n",
    "print(attention_mask_train.shape)\n",
    "print(polarite_train.shape)\n",
    "#train_dataset = TensorDataset(input_ids_train, attention_mask, polarite)\n",
    "\n",
    "\n",
    "input_ids_test, attention_mask_test, polarite_test = preprocess_data(critique_test, polarite_test, tokenizer)\n",
    "print(input_ids_test.shape)\n",
    "print(attention_mask_test.shape)\n",
    "print(polarite_test.shape)\n",
    "#test_dataset = TensorDataset(input_ids, attention_mask, polarite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Déplacez le modèle sur le GPU s'il est disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Déplacez les données d'entraînement sur le GPU\n",
    "input_ids_train_, attention_mask_train_, polarite_train_ = input_ids_train.to(device), attention_mask_train.to(device), polarite_train.to(device)\n",
    "train_dataset = TensorDataset(input_ids_train_, attention_mask_train_, polarite_train_)\n",
    "\n",
    "# Déplacez les données de test sur le GPU\n",
    "input_ids_test_, attention_mask_test_, polarite_test_ = input_ids_test.to(device), attention_mask_test.to(device), polarite_test.to(device)\n",
    "test_dataset = TensorDataset(input_ids_test_ , attention_mask_test_, polarite_test_)\n",
    "\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/2500 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 14.54 GiB of which 2.75 MiB is free. Process 3997949 has 14.53 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 94.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/onyxia/work/python_ds_film/modelisation.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m input_ids, attention_mask, labels \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device), attention_mask\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://user-mastoche-491498-0.user.lab.sspcloud.fr/home/onyxia/work/python_ds_film/modelisation.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:1063\u001b[0m, in \u001b[0;36mCamembertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1063\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1064\u001b[0m     input_ids,\n\u001b[1;32m   1065\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1066\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1067\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1068\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1069\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1070\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1071\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1072\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1073\u001b[0m )\n\u001b[1;32m   1074\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1075\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:888\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    879\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    881\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    882\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    883\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    887\u001b[0m )\n\u001b[0;32m--> 888\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    889\u001b[0m     embedding_output,\n\u001b[1;32m    890\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    891\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    892\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    893\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    894\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    895\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    896\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    897\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    898\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    899\u001b[0m )\n\u001b[1;32m    900\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:538\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    528\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    529\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         output_attentions,\n\u001b[1;32m    536\u001b[0m     )\n\u001b[1;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    539\u001b[0m         hidden_states,\n\u001b[1;32m    540\u001b[0m         attention_mask,\n\u001b[1;32m    541\u001b[0m         layer_head_mask,\n\u001b[1;32m    542\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    543\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    544\u001b[0m         past_key_value,\n\u001b[1;32m    545\u001b[0m         output_attentions,\n\u001b[1;32m    546\u001b[0m     )\n\u001b[1;32m    548\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:427\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    425\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    432\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:354\u001b[0m, in \u001b[0;36mCamembertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    345\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    346\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    353\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 354\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    355\u001b[0m         hidden_states,\n\u001b[1;32m    356\u001b[0m         attention_mask,\n\u001b[1;32m    357\u001b[0m         head_mask,\n\u001b[1;32m    358\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    359\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    360\u001b[0m         past_key_value,\n\u001b[1;32m    361\u001b[0m         output_attentions,\n\u001b[1;32m    362\u001b[0m     )\n\u001b[1;32m    363\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    364\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:234\u001b[0m, in \u001b[0;36mCamembertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 234\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[1;32m    236\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    238\u001b[0m use_cache \u001b[39m=\u001b[39m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 14.54 GiB of which 2.75 MiB is free. Process 3997949 has 14.53 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 94.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    # Initialize tqdm for progress bar\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=num_batches, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch')\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.unsqueeze(1))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update tqdm bar with current loss\n",
    "        progress_bar.set_postfix({'loss': loss.item(), 'avg_loss': total_loss / (batch_idx + 1)})\n",
    "\n",
    "        # Optional: Simulate time delay to show the progress bar\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Print epoch-level training statistics\n",
    "    avg_epoch_loss = total_loss / num_batches\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Average Training Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "# Additional code for model evaluation or saving the trained model can be added here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(logits.argmax(dim=1).tolist())\n",
    "        true_labels.extend(labels.squeeze().tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(true_labels, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
